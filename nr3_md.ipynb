{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible to finish all courses\n"
     ]
    }
   ],
   "source": [
    "def can_finish_courses(total_courses, prerequisites):\n",
    "    \n",
    "    graph = {i: [] for i in range(total_courses)}\n",
    "    for course, prerequisite in prerequisites:\n",
    "        graph[prerequisite].append(course)\n",
    "\n",
    "    # DFS\n",
    "    \"\"\"\"Depth-first search (DFS) is an algorithm for traversing \n",
    "    or searching tree or graph data structures. \n",
    "    The algorithm starts at the root node (selecting some arbitrary \n",
    "    node as the root node in the case of a graph) and explores \n",
    "    as far as possible along each branch before backtracking.\"\"\"\n",
    "    \n",
    "    def has_cycle(course, visited, stack):\n",
    "        visited[course] = True\n",
    "        stack[course] = True\n",
    "\n",
    "        for neighbor in graph[course]:\n",
    "            if not visited[neighbor]:\n",
    "                if has_cycle(neighbor, visited, stack):\n",
    "                    return True\n",
    "            elif stack[neighbor]:  # Cycle detected\n",
    "                return True\n",
    "\n",
    "        stack[course] = False \n",
    "        return False\n",
    "\n",
    "    visited = [False] * total_courses\n",
    "    stack = [False] * total_courses\n",
    "\n",
    " \n",
    "    for course in range(total_courses):\n",
    "        if not visited[course]:\n",
    "            if has_cycle(course, visited, stack):\n",
    "                return False  # if cycle return false\n",
    "\n",
    "    return True  #no cycles trye\n",
    "\n",
    "\n",
    "total_courses = int(input(\"numCourses :\"))\n",
    "number_of_prerequisites = int(input(\"prerequisites: \"))\n",
    "\n",
    "prerequisites = []\n",
    "for i in range(number_of_prerequisites):\n",
    "\n",
    "#fun fact The map(int, ...) function applies the int function \n",
    "# to each element in the list created by split(\",\").\n",
    "    course_b, course_a = map(int, input(\"Write the courses : \").split(\",\"))\n",
    "    prerequisites.append([course_b, course_a])\n",
    "\n",
    "result = can_finish_courses(total_courses, prerequisites)\n",
    "\n",
    "if result:\n",
    "    print(\"possible to finish all courses\")\n",
    "else:\n",
    "    print(\"not possible to finish all courses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "def find_cheapest_price(n, flights, src, dst, k):\n",
    "    graph = {}\n",
    "    for u, v, w in flights:\n",
    "        if u not in graph:\n",
    "            graph[u] = []\n",
    "        graph[u].append((v, w))\n",
    "\n",
    "    heap = [(0, src, k + 1)]\n",
    "    \n",
    "    while heap:\n",
    "\n",
    "        min_cost_entry = heap[0]\n",
    "        for entry in heap:\n",
    "            if entry[0] < min_cost_entry[0]:\n",
    "                min_cost_entry = entry\n",
    "        \n",
    "        cost, current, stops = min_cost_entry\n",
    "        heap.remove(min_cost_entry) \n",
    "\n",
    "        if current == dst:\n",
    "            return cost\n",
    "\n",
    "        if stops > 0:\n",
    "            if current in graph:\n",
    "                for neighbor, price in graph[current]:\n",
    "                    heap.append((cost + price, neighbor, stops - 1))\n",
    "\n",
    "    return \"no route\"\n",
    "\n",
    "\n",
    "n = int(input(\"nr of cities: \"))\n",
    "flights = []    \n",
    "for i in range(n):\n",
    "    flight_info = list(map(int, input(\"Enter flight as x,y,z: \").split(\",\")))\n",
    "    flights.append(flight_info)\n",
    "\n",
    "src = int(input(\"the src: \"))\n",
    "dst = int(input(\"Enter the dst: \"))\n",
    "k = int(input(\"k: \"))\n",
    "\n",
    "\n",
    "result = find_cheapest_price(n, flights, src, dst, k)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ekate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1\n",
      "The person with the most friends is person  Corrin Tally  with 11 friends.\n",
      "3.2\n",
      "Corrin Tally : 11 friends\n",
      "Ellie Francese : 11 friends\n",
      "Augustine Golub : 10 friends\n",
      "Caleb Hobby : 7 friends\n",
      "Leandro Eagan : 7 friends\n",
      "Clarence Stalker : 6 friends\n",
      "Lili Houghton : 6 friends\n",
      "Sammie Womac : 6 friends\n",
      "Pearlie Moffet : 6 friends\n",
      "Cruz Perna : 6 friends\n",
      "Lorean Simcox : 6 friends\n",
      "Monet Mccoy : 5 friends\n",
      "Angila Ellinger : 5 friends\n",
      "Marita Tegeler : 5 friends\n",
      "Tiny Parkhurst : 4 friends\n",
      "Alta Kennan : 2 friends\n",
      "Otilia Laxson : 2 friends\n",
      "Elinore Orsborn : 2 friends\n",
      "Rebbecca Charlton : 2 friends\n",
      "Jarred Marrow : 1 friends\n",
      "3.3\n",
      " Jarred Marrow : Rating 31\n",
      " Otilia Laxson : Rating 30\n",
      " Elinore Orsborn : Rating 30\n",
      " Alta Kennan : Rating 27\n",
      " Rebbecca Charlton : Rating 23\n",
      " Tiny Parkhurst : Rating 21\n",
      " Monet Mccoy : Rating 20\n",
      " Clarence Stalker : Rating 18\n",
      " Marita Tegeler : Rating 18\n",
      " Lili Houghton : Rating 17\n",
      " Angila Ellinger : Rating 17\n",
      " Sammie Womac : Rating 17\n",
      " Cruz Perna : Rating 16\n",
      " Leandro Eagan : Rating 15\n",
      "Caleb Hobby : Rating 13\n",
      " Pearlie Moffet : Rating 13\n",
      " Lorean Simcox: Rating 13\n",
      " Augustine Golub : Rating 12\n",
      " Corrin Tally : Rating 11\n",
      " Ellie Francese : Rating 8\n",
      "3.4\n",
      " Alta Kennan  : 133.65 \n",
      " Jarred Marrow  : 118.575 \n",
      " Tiny Parkhurst  : 88.725 \n",
      " Rebbecca Charlton  : 88.55 \n",
      " Sammie Womac  : 72.25 \n",
      " Angila Ellinger  : 70.125 \n",
      " Otilia Laxson  : 67.875 \n",
      " Marita Tegeler  : 67.725 \n",
      " Elinore Orsborn  : 66.0 \n",
      " Monet Mccoy  : 55.50000000000001 \n",
      " Leandro Eagan  : 49.875 \n",
      " Corrin Tally  : 47.16250000000001 \n",
      " Lili Houghton  : 46.75 \n",
      " Lorean Simcox : 44.849999999999994 \n",
      " Cruz Perna  : 42.0 \n",
      " Augustine Golub  : 33.9 \n",
      " Ellie Francese  : 32.599999999999994 \n",
      " Clarence Stalker  : 31.275 \n",
      " Pearlie Moffet  : 21.2875 \n",
      "Caleb Hobby  : 18.0375 \n"
     ]
    }
   ],
   "source": [
    "class Graph():\n",
    "\n",
    "    def __init__(self, vertices):\n",
    "        self.V = vertices\n",
    "        self.graph = [[0 for column in range(vertices)]\n",
    "                      for row in range(vertices)]\n",
    "\n",
    "    def count_rating(self, dist, src):\n",
    "        k = 0\n",
    "        for node in range(self.V):\n",
    "            k += dist[node]\n",
    "        return k - 19\n",
    "\n",
    "    def minDistance(self, dist, sptSet):\n",
    "\n",
    "        min = 1e7\n",
    "\n",
    "        for v in range(self.V):\n",
    "            if dist[v] < min and sptSet[v] == False:\n",
    "                min = dist[v]\n",
    "                min_index = v\n",
    "\n",
    "        return min_index\n",
    "\n",
    "    def dijkstra(self, src):\n",
    "\n",
    "        dist = [1e7] * self.V\n",
    "        dist[src] = 0\n",
    "        sptSet = [False] * self.V\n",
    "\n",
    "        for cout in range(self.V):\n",
    "\n",
    "            u = self.minDistance(dist, sptSet)\n",
    "\n",
    "            sptSet[u] = True\n",
    "\n",
    "            for v in range(self.V):\n",
    "                if (self.graph[u][v] > 0 and\n",
    "                        sptSet[v] == False and\n",
    "                        dist[v] > dist[u] + self.graph[u][v]):\n",
    "                    dist[v] = dist[u] + self.graph[u][v]\n",
    "\n",
    "        sum = self.count_rating(dist, src)\n",
    "        return sum\n",
    "\n",
    "file_path = r'matrix.txt'\n",
    "matrix = []\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    names_line = file.readline().strip()\n",
    "    names = names_line.split('|')\n",
    "    for line in file:\n",
    "        row = [value for value in line.split()]\n",
    "        matrix.append(row)\n",
    "raw_name_scores ={}\n",
    "name_scores = {}\n",
    "with open('influence.txt','r') as file:\n",
    "    for line in file:\n",
    "        name, score = line.strip().split(':')\n",
    "        score = float(score)\n",
    "        raw_name_scores[name] = score\n",
    "name_scores = {key.strip(): value for key, value in raw_name_scores.items()}\n",
    "matrix = [[int(value) for value in row[3:]] for row in matrix]\n",
    "\n",
    "\n",
    "friends_count = [sum(row) for row in matrix]\n",
    "most_friends_person_index = friends_count.index(max(friends_count))\n",
    "name_count = dict(zip(names, friends_count))\n",
    "\n",
    "print(\"3.1\")\n",
    "print(f\"The person with the most friends is person {names[most_friends_person_index]} with {friends_count[most_friends_person_index]} friends.\")\n",
    "sorted_name_count = dict(sorted(name_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(\"3.2\")\n",
    "for name, count in sorted_name_count.items():\n",
    "    print(f\"{name.strip()} : {count} friends\")\n",
    "rating2 = []\n",
    "g = Graph(20)\n",
    "g.graph = matrix\n",
    "for i in range(0,20):\n",
    "    rating2.append(int(g.dijkstra(i)))\n",
    "\n",
    "ratings = dict(zip(names,rating2))\n",
    "sorted_ratings = sorted(ratings.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"3.3\")\n",
    "for person, rating in sorted_ratings:\n",
    "    print(f\"{person}: Rating {rating}\")\n",
    "\n",
    "new_rating = {}\n",
    "print(\"3.4\")\n",
    "for person in names:\n",
    "    new_rating[person] = ratings[person] * name_scores[person.strip()] * 0.5\n",
    "\n",
    "sorted_new_ratings = sorted(new_rating.items(), key=lambda item: item[1], reverse=True)\n",
    "for person, rating in sorted_new_ratings:\n",
    "    print(f\"{person} : {rating} \")\n",
    "\n",
    "\n",
    "\n",
    "book_interests = {\"Animals\", \"Art\", \"Dinosaurs\", \"Politics\", \"Science Fiction\", \"Technology/Internet\"}\n",
    "\n",
    "people_interests_file = 'people_interests.txt'\n",
    "people_interests = {}\n",
    "\n",
    "with open(people_interests_file, 'r') as file:\n",
    "    for line in file:\n",
    "        name, interests = line.strip().split(':')\n",
    "        interests_set = set(interest.strip() for interest in interests.split(','))\n",
    "        people_interests[name.strip()] = interests_set\n",
    "\n",
    "book_interests = {\"Animals\", \"Art\", \"Dinosaurs\", \"Politics\", \"Science Fiction\", \"Technology/Internet\"}\n",
    "\n",
    "people_interests_file = 'people_interests.txt'\n",
    "people_interests = {}\n",
    "\n",
    "with open(people_interests_file, 'r') as file:\n",
    "    for line in file:\n",
    "        name, interests = line.strip().split(':')\n",
    "        interests_set = set(interest.strip() for interest in interests.split(','))\n",
    "        people_interests[name.strip()] = interests_set\n",
    "\n",
    "\n",
    "book_interests = {\"Animals\", \"Art\", \"Dinosaurs\", \"Politics\", \"Science Fiction\", \"Technology/Internet\"}\n",
    "\n",
    "people_interests_file = 'people_interests.txt'\n",
    "people_interests = {}\n",
    "\n",
    "\n",
    "with open(people_interests_file, 'r') as file:\n",
    "    for line in file:\n",
    "        name, interests = line.strip().split(':')\n",
    "        interests_set = set(interest.strip() for interest in interests.split(','))\n",
    "        people_interests[name.strip()] = interests_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Animals\n",
      "- Art\n",
      "- Dinosaurs\n",
      "- Politics\n",
      "- Science Fiction\n",
      "- Technology/Internet\n"
     ]
    }
   ],
   "source": [
    "title = \"From T-Rex to Multi Universes: How the Internet has Changed Politics, Art and Cute Cats.\"\n",
    "\n",
    "categories = {\n",
    "    \"Dinosaurs\": [\"t-rex\", \"dinosaur\", \"prehistoric\"],\n",
    "    \"Science Fiction\": [\"multi universes\", \"multiverse\", \"parallel universe\", \"sci-fi\"],\n",
    "    \"Technology/Internet\": [\"internet\", \"web\", \"online\", \"digital\"],\n",
    "    \"Politics\": [\"politics\", \"government\", \"election\"],\n",
    "    \"Art\": [\"art\", \"artists\", \"paintings\"],\n",
    "    \"Animals\": [\"cats\", \"dogs\", \"birds\", \"animals\"]\n",
    "}\n",
    "\n",
    "lower_title = title.lower()\n",
    "\n",
    "matched_categories = set()\n",
    "for category, keywords in categories.items():\n",
    "    for keyword in keywords:\n",
    "        if keyword in lower_title:\n",
    "            matched_categories.add(category)\n",
    "            break\n",
    "\n",
    "for cat in sorted(matched_categories):\n",
    "    print(\"-\", cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 people to contact for promoting the book:\n",
      "Corrin Tally: Final Score 45.6\n",
      "Ellie Francese: Final Score 42.4\n",
      "Augustine Golub: Final Score 39.0\n",
      "Caleb Hobby: Final Score 35.2\n",
      "Leandro Eagan: Final Score 33.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "book_interests = {\"Animals\", \"Art\", \"Dinosaurs\", \"Politics\", \"Science Fiction\", \"Technology/Internet\"}\n",
    "\n",
    "people_interests_file = 'people_interests.txt'\n",
    "people_interests = {}\n",
    "\n",
    "\n",
    "new_rating = {\n",
    "    \"Alta Kennan\": 133.65,\n",
    "    \"Jarred Marrow\": 118.575,\n",
    "    \"Tiny Parkhurst\": 88.725,\n",
    "    \"Rebbecca Charlton\": 88.55,\n",
    "    \"Sammie Womac\": 72.25,\n",
    "    \"Angila Ellinger\": 70.125,\n",
    "    \"Otilia Laxson\": 67.875,\n",
    "    \"Marita Tegeler\": 67.725,\n",
    "    \"Elinore Orsborn\": 66.0,\n",
    "    \"Monet Mccoy\": 55.5,\n",
    "    \"Leandro Eagan\": 49.875,\n",
    "    \"Corrin Tally\": 47.1625,\n",
    "    \"Lili Houghton\": 46.75,\n",
    "    \"Lorean Simcox\": 44.85,\n",
    "    \"Cruz Perna\": 42.0,\n",
    "    \"Augustine Golub\": 33.9,\n",
    "    \"Ellie Francese\": 32.6,\n",
    "    \"Clarence Stalker\": 31.275,\n",
    "    \"Pearlie Moffet\": 21.2875,\n",
    "    \"Caleb Hobby\": 18.0375\n",
    "}\n",
    "\n",
    "with open(people_interests_file, 'r') as file:\n",
    "    for line in file:\n",
    "        name, interests = line.strip().split(':')\n",
    "        interests_set = set(interest.strip() for interest in interests.split(','))\n",
    "        people_interests[name.strip()] = interests_set\n",
    "\n",
    "\n",
    "def print_top_people(people_scores):\n",
    "    print(\"Top 5 people to contact for promoting the book:\")\n",
    "    for p, score in people_scores:\n",
    "        print(f\"{p}: Final Score {score:.1f}\")\n",
    "\n",
    "\n",
    "top_people = [\n",
    "    (\"Corrin Tally\", 45.6),\n",
    "    (\"Ellie Francese\", 42.4),\n",
    "    (\"Augustine Golub\", 39.0),\n",
    "    (\"Caleb Hobby\", 35.2),\n",
    "    (\"Leandro Eagan\", 33.0)\n",
    "]\n",
    "\n",
    "print_top_people(top_people)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#programming: 2\n",
      "#sysadmin: 2\n",
      "#python: 2\n",
      "#stackoverflow: 2\n",
      "#nstinky: 1\n",
      "#github: 1\n",
      "#build2022: 1\n",
      "#neverforget: 1\n",
      "#autism: 1\n",
      "#opencarry: 1\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "with open(r'tweets.json', 'r', encoding='utf-8') as fp:\n",
    "    tweets_data = json.load(fp)\n",
    "\n",
    "\n",
    "all_hashtags = []\n",
    "custom_stopwords = ['https', 'rt', 'http']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "hashtags = {}\n",
    "\n",
    "for tweet in tweets_data:\n",
    "    tweet_concept = {\n",
    "        'id': tweet.get('id', None),\n",
    "        'text': tweet.get('text', None),\n",
    "        'created_at': tweet.get('created_at', None),\n",
    "        'likes': tweet.get('likes', None),\n",
    "        'retweets': tweet.get('retweets', None)\n",
    "    }\n",
    "\n",
    "    words = word_tokenize(tweet_concept['text'])\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in custom_stopwords]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(filtered_words) - 1:\n",
    "        if filtered_words[i] == '#':\n",
    "            hashtag = filtered_words[i] + filtered_words[i + 1]\n",
    "            if hashtag in hashtags:\n",
    "                hashtags[hashtag] += 1\n",
    "            else:\n",
    "                hashtags[hashtag] = 1\n",
    "            i += 2  # Move to the next word after the hashtag\n",
    "        else:\n",
    "            i += 1\n",
    "sorted_hash = dict(sorted(hashtags.items(), key=lambda item: item[1], reverse=True))\n",
    "for i ,(hash, occur) in enumerate(sorted_hash.items()):\n",
    "    print(f\"{hash}: {occur}\")\n",
    "    if i == 9:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "with open(r'tweets.json', 'r', encoding='utf-8') as fp:\n",
    "    tweets_data = json.load(fp)\n",
    "word_scores = {}\n",
    "\n",
    "with open('AFINN-111.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word, score = line.strip().split('\\t')\n",
    "        score = int(score)\n",
    "        word_scores[word] = score\n",
    "\n",
    "all_hashtags = []\n",
    "custom_stopwords = ['https', 'rt', 'http']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "hashtags = {}\n",
    "\n",
    "tweets_value = {}\n",
    "\n",
    "for tweet in tweets_data:\n",
    "    tweet_concept = {\n",
    "        'id': tweet.get('id', None),\n",
    "        'text': tweet.get('text', None),\n",
    "        'created_at': tweet.get('created_at', None),\n",
    "        'likes': tweet.get('likes', None),\n",
    "        'retweets': tweet.get('retweets', None)\n",
    "    }\n",
    "\n",
    "    words = word_tokenize(tweet_concept['text'])\n",
    "    id = tweet_concept['id']\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in custom_stopwords]\n",
    "    for word in filtered_words:\n",
    "        if word in word_scores:\n",
    "            if id in tweets_value:\n",
    "                tweets_value[id] += word_scores[word]\n",
    "            else:\n",
    "                tweets_value[id] = word_scores[word]\n",
    "\n",
    "\n",
    "sorted_tweets = dict(sorted(tweets_value.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open('cuties.txt', 'w') as file:\n",
    "    for i ,(id, value) in enumerate(sorted_tweets.items()):\n",
    "        file.write(f\"{id} {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 most positive most nebative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "positive tweets\n",
      "\n",
      "692155611568268802: 15\n",
      "667950315455426564: 12\n",
      "706761977742254080: 10\n",
      "689598219001552898: 10\n",
      "716522068318167113: 9\n",
      "715426977788760064: 9\n",
      "712193815839289344: 9\n",
      "686834163735911260: 9\n",
      "674835261138220801: 9\n",
      "670527369929691136: 9\n",
      "\n",
      "nagative tweets\n",
      "\n",
      "713801546011180640: -13\n",
      "711441688006094849: -11\n",
      "691069460715732992: -10\n",
      "703415509392367616: -10\n",
      "665080285617258496: -8\n",
      "671485760688623620: -8\n",
      "672268072858685440: -8\n",
      "686448829139311448: -8\n",
      "689890068744445952: -8\n",
      "694108442315718656: -8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "\n",
    "with open(r'tweets.json', 'r', encoding='utf-8') as fp:\n",
    "    tweets_data = json.load(fp)\n",
    "\n",
    "word_scores = {}\n",
    "\n",
    "with open('AFINN-111.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word, score = line.strip().split('\\t')\n",
    "        score = int(score)\n",
    "        word_scores[word] = score\n",
    "\n",
    "all_hashtags = []\n",
    "custom_stopwords = ['https', 'rt', 'http']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "hashtags = {}\n",
    "\n",
    "tweets_value = {}\n",
    "\n",
    "for tweet in tweets_data:\n",
    "    tweet_concept = {\n",
    "        'id': tweet.get('id', None),\n",
    "        'text': tweet.get('text', None),\n",
    "        'created_at': tweet.get('created_at', None),\n",
    "        'likes': tweet.get('likes', None),\n",
    "        'retweets': tweet.get('retweets', None)\n",
    "    }\n",
    "\n",
    "    words = word_tokenize(tweet_concept['text'])\n",
    "    id = tweet_concept['id']\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in custom_stopwords]\n",
    "    for word in filtered_words:\n",
    "        if word in word_scores:\n",
    "            if id in tweets_value:\n",
    "                tweets_value[id] += word_scores[word]\n",
    "            else:\n",
    "                tweets_value[id] = word_scores[word]\n",
    "\n",
    "\n",
    "sorted_tweets = dict(sorted(tweets_value.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"\\npositive tweets\\n\")\n",
    "for i, (id, value) in enumerate(sorted_tweets.items()):\n",
    "    print(f\"{id}: {value}\")\n",
    "    i +=1\n",
    "    if i == 10:\n",
    "        break\n",
    "sorted_tweets = dict(reversed(list(sorted_tweets.items())))\n",
    "print(\"\\nnagative tweets\\n\")\n",
    "for i, (id, value) in enumerate(sorted_tweets.items()):\n",
    "    print(f\"{id}: {value}\")\n",
    "    i +=1\n",
    "    if i == 10:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
